---
title: On Convergence of Emphatic Temporal-Difference Learning
abstract: We consider emphatic temporal-difference learning algorithms for policy
  evaluation in discounted Markov decision processes with finite spaces. Such algorithms
  were recently proposed by Sutton, Mahmood, and White (2015) as an improved solution
  to the problem of divergence of off-policy temporal-difference learning with linear
  function approximation. We present in this paper the first convergence proofs for
  two emphatic algorithms, ETD(λ) and ELSTD(λ). We prove, under general off-policy
  conditions, the convergence in L^1 for ELSTD(λ) iterates, and the almost sure convergence
  of the approximate value functions calculated by both algorithms using a single
  infinitely long trajectory. Our analysis involves new techniques with applications
  beyond emphatic algorithms leading, for example, to the first proof that standard
  TD(λ) also converges under off-policy training for λsufficiently large.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: Yu15
month: 0
tex_title: On Convergence of Emphatic Temporal-Difference Learning
firstpage: 1724
lastpage: 1751
page: 1724-1751
sections: 
author:
- given: H.
  family: Yu
date: 2015-06-26
address: Paris, France
publisher: PMLR
container-title: Proceedings of The 28th Conference on Learning Theory
volume: '40'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 26
pdf: http://proceedings.mlr.press/v40/Yu15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
