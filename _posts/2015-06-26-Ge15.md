---
title: Escaping From Saddle Points --- Online Stochastic Gradient for Tensor Decomposition
abstract: We analyze stochastic gradient descent for optimizing non-convex functions.
  In many cases for non-convex functions the goal is to find a reasonable local minimum,
  and the main concern is that gradient updates are trapped in \em saddle points.
  In this paper we identify \em strict saddle property for non-convex problem that
  allows for efficient optimization. Using this property we show that from an \em
  arbitrary starting point,  stochastic gradient descent converges to a local minimum
  in a polynomial number of iterations. To the best of our knowledge this is the first
  work that gives \em global convergence guarantees for stochastic gradient descent
  on non-convex functions with exponentially many local minima and saddle points.
  Our analysis can be applied to orthogonal tensor decomposition, which is widely
  used in learning a rich class of  latent variable models. We propose a new optimization
  formulation for the tensor decomposition problem that has strict saddle property.
  As a result we get the first online algorithm for orthogonal tensor decomposition
  with global convergence guarantee.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: Ge15
month: 0
firstpage: 797
lastpage: 842
page: 797-842
sections: 
author:
- given: Rong
  family: Ge
- given: Furong
  family: Huang
- given: Chi
  family: Jin
- given: Yang
  family: Yuan
date: 2015-06-26
address: Paris, France
publisher: PMLR
container-title: Proceedings of The 28th Conference on Learning Theory
volume: '40'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 26
pdf: http://proceedings.mlr.press/v40/Ge15/Ge15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
