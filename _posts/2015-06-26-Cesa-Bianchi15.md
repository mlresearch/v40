---
title: On the Complexity of Learning with Kernels
abstract: A well-recognized limitation of kernel learning is the requirement to handle
  a kernel matrix, whose size is quadratic in the number of training examples. Many
  methods have been proposed to reduce this computational cost, mostly by using a
  subset of the kernel matrix entries, or some form of low-rank matrix approximation,
  or a random projection method. In this paper, we study lower bounds on the error
  attainable by such methods as a function of the number of entries observed in the
  kernel matrix or the rank of an approximate kernel matrix. We show that there are
  kernel learning problems where no such method will lead to non-trivial computational
  savings. Our results also quantify how the problem difficulty depends on parameters
  such as the nature of the loss function, the regularization parameter, the norm
  of the desired predictor, and the kernel matrix rank. Our results also suggest cases
  where more efficient kernel learning might be possible.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: Cesa-Bianchi15
month: 0
tex_title: On the Complexity of Learning with Kernels
firstpage: 297
lastpage: 325
page: 297-325
sections: 
author:
- given: Nicol√≤
  family: Cesa-Bianchi
- given: Yishay
  family: Mansour
- given: Ohad
  family: Shamir
date: 2015-06-26
address: Paris, France
publisher: PMLR
container-title: Proceedings of The 28th Conference on Learning Theory
volume: '40'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 26
pdf: http://proceedings.mlr.press/v40/Cesa-Bianchi15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
