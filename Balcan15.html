<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Efficient Representations for Lifelong Learning and Autoencoding | COLT 2015 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Efficient Representations for Lifelong Learning and Autoencoding">

  <meta name="citation_author" content="Balcan, Maria-Florina">

  <meta name="citation_author" content="Blum, Avrim">

  <meta name="citation_author" content="Vempala, Santosh">

<meta name="citation_publication_date" content="2015">
<meta name="citation_conference_title" content="Proceedings of The 28th Conference on Learning Theory">
<meta name="citation_firstpage" content="191">
<meta name="citation_lastpage" content="210">
<meta name="citation_pdf_url" content="Balcan15.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Efficient Representations for Lifelong Learning and Autoencoding</h1>

	<div id="authors">
	
		Maria-Florina Balcan,
	
		Avrim Blum,
	
		Santosh Vempala
	<br />
	</div>
	<div id="info">
		Proceedings of The 28th Conference on Learning Theory,
		pp. 191–210, 2015
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		It has been a long-standing goal in machine learning, as well as in AI more generally, to develop life-long learning systems that learn many different tasks over time, and reuse insights from tasks learned, “learning to learn” as they do so. In this work we pose and provide efficient algorithms for several natural theoretical formulations of this goal. Specifically, we consider the problem of learning many different target functions over time, that share certain commonalities that are initially unknown to the learning algorithm. Our aim is to learn new internal representations as the algorithm learns new target functions, that capture this commonality and allow subsequent learning tasks to be solved more efficiently and from less data. We develop efficient algorithms for two very different kinds of commonalities that target functions might share: one based on learning common low-dimensional and unions of low-dimensional subspaces and one based on learning nonlinear Boolean combinations of features. Our algorithms for learning Boolean feature combinations additionally have a dual interpretation, and can be viewed as giving an efficient procedure for constructing near-optimal sparse Boolean autoencoders under a natural “anchor-set” assumption.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="Balcan15.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
