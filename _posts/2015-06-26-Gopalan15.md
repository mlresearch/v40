---
title: Thompson Sampling for Learning Parameterized Markov Decision Processes
abstract: We consider reinforcement learning in parameterized Markov Decision Processes
  (MDPs), where the parameterization may induce correlation across transition probabilities
  or rewards. Consequently, observing a particular state transition might yield useful
  information about other, unobserved, parts of the MDP. We present a version of Thompson
  sampling for parameterized reinforcement learning problems, and derive a frequentist
  regret bound for priors over general parameter spaces. The result shows that the
  number of instants where suboptimal actions are chosen scales logarithmically with
  time, with high probability. It holds for prior distributions that put significant
  probability near the true model, without any additional, specific closed-form structure
  such as conjugate or product-form priors. The constant factor in the logarithmic
  scaling encodes the information complexity of learning the MDP in terms of the Kullback-Leibler
  geometry of the parameter space.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: Gopalan15
month: 0
tex_title: "{Thompson Sampling for Learning Parameterized Markov Decision Processes}"
firstpage: 861
lastpage: 898
page: 861-898
sections: 
author:
- given: Aditya
  family: Gopalan
- given: Shie
  family: Mannor
date: 2015-06-26
address: Paris, France
publisher: PMLR
container-title: Proceedings of The 28th Conference on Learning Theory
volume: '40'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 26
pdf: http://proceedings.mlr.press/v40/Gopalan15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
