---
title: 'Sequential Information Maximization: When is Greedy Near-optimal?'
abstract: 'Optimal information gathering is a central challenge in machine learning
  and science in general.  A common objective that quantifies the usefulness of observations
  is Shannonâ€™s mutual information, defined w.r.t. a probabilistic model. Greedily
  selecting observations that maximize the mutual information is the method of choice
  in numerous applications, ranging from Bayesian experimental design to automated
  diagnosis, to active learning in Bayesian models. Despite its importance and widespread
  use in applications, little is known about the theoretical properties of sequential
  information maximization, in particular under noisy observations.  In this paper,
  we analyze the widely used greedy policy for this task, and identify problem instances
  where it provides provably near-maximal utility, even in the challenging setting
  of persistent noise.  Our results depend on a natural separability condition associated
  with a channel injecting noise into the observations. We also identify examples
  where this separability parameter is necessary in the bound: if it is too small,
  then the greedy policy fails to select informative tests.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: Chen15b
month: 0
firstpage: 338
lastpage: 363
page: 338-363
sections: 
author:
- given: Yuxin
  family: Chen
- given: S. Hamed
  family: Hassani
- given: Amin
  family: Karbasi
- given: Andreas
  family: Krause
date: 2015-06-26
address: Paris, France
publisher: PMLR
container-title: Proceedings of The 28th Conference on Learning Theory
volume: '40'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 26
pdf: http://proceedings.mlr.press/v40/Chen15b/Chen15b.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
