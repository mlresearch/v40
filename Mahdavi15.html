<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Lower and Upper Bounds on the Generalization of Stochastic Exponentially Concave Optimization | COLT 2015 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Lower and Upper Bounds on the Generalization of  Stochastic   Exponentially Concave Optimization">

  <meta name="citation_author" content="Mahdavi, Mehrdad">

  <meta name="citation_author" content="Zhang, Lijun">

  <meta name="citation_author" content="Jin, Rong">

<meta name="citation_publication_date" content="2015">
<meta name="citation_conference_title" content="Proceedings of The 28th Conference on Learning Theory">
<meta name="citation_firstpage" content="1305">
<meta name="citation_lastpage" content="1320">
<meta name="citation_pdf_url" content="Mahdavi15.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Lower and Upper Bounds on the Generalization of Stochastic Exponentially Concave Optimization</h1>

	<div id="authors">
	
		Mehrdad Mahdavi,
	
		Lijun Zhang,
	
		Rong Jin
	<br />
	</div>
	<div id="info">
		Proceedings of The 28th Conference on Learning Theory,
		pp. 1305â€“1320, 2015
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		In this paper we derive <em>high probability</em> lower and upper bounds on the excess risk of stochastic optimization of exponentially concave loss functions. Exponentially concave loss functions encompass several fundamental problems in machine learning such as squared loss in linear regression, logistic loss in classification, and negative logarithm loss in portfolio management. We demonstrate an <span class="math">\(O(d \log T/T)\)</span> upper bound on the excess risk of stochastic online Newton step algorithm, and an <span class="math">\(O(d/T)\)</span> lower bound on the excess risk of any stochastic optimization method for <em>squared loss</em>, indicating that the obtained upper bound is optimal up to a logarithmic factor. The analysis of upper bound is based on recent advances in concentration inequalities for bounding self-normalized martingales, which is interesting by its own right, and the proof technique used to achieve the lower bound is a probabilistic method and relies on an information-theoretic minimax analysis.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="Mahdavi15.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
