---
title: Convex Risk Minimization and Conditional Probability Estimation
abstract: 'This paper proves, in very general settings, that convex risk minimization
  is a procedure to select a unique conditional probability model determined by the
  classification problem. Unlike most previous work, we give results that are general
  enough to include cases in which no minimum exists, as occurs typically, for instance,
  with standard boosting algorithms. Concretely, we first show that any sequence of
  predictors minimizing convex risk over the source distribution will converge to
  this unique model when the class of predictors is linear (but potentially of infinite
  dimension). Secondly, we show the same result holds for \emphempirical risk minimization
  whenever this class of predictors is finite dimensional, where the essential technical
  contribution is a norm-free generalization bound. '
layout: inproceedings
series: Proceedings of Machine Learning Research
id: Telgarsky15
month: 0
tex_title: Convex Risk Minimization and Conditional Probability Estimation
firstpage: 1629
lastpage: 1682
page: 1629-1682
sections: 
author:
- given: Matus
  family: Telgarsky
- given: Miroslav
  family: Dud√≠k
- given: Robert
  family: Schapire
date: 2015-06-26
address: Paris, France
publisher: PMLR
container-title: Proceedings of The 28th Conference on Learning Theory
volume: '40'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 26
pdf: http://proceedings.mlr.press/v40/Telgarsky15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
