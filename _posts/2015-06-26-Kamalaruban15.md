---
title: Exp-Concavity of Proper Composite Losses
abstract: The goal of online prediction with expert advice is to find a decision strategy
  which will perform almost as well as the best expert in a given pool of experts,
  on any sequence of outcomes. This problem has been widely studied and O(\sqrtT)
  and O(\logT) regret bounds can be achieved for convex losses and strictly convex
  losses with bounded first and second derivatives respectively. In special cases
  like the Aggregating Algorithm with mixable losses and the Weighted Average Algorithm
  with exp-concave losses, it is possible to achieve O(1) regret bounds. But mixability
  and exp-concavity are roughly equivalent under certain conditions. Thus by understanding
  the underlying relationship between these two notions we can gain the best of both
  algorithms (strong theoretical performance guarantees of the Aggregating Algorithm
  and the computational efficiency of the Weighted Average Algorithm). In this paper
  we provide a complete characterization of the exp-concavity of any proper composite
  loss. Using this characterization and the mixability condition of proper losses,
  we show that it is possible to transform (re-parameterize) any β-mixable binary
  proper loss into a β-exp-concave composite loss with the same β. In the multi-class
  case, we propose an approximation approach for this transformation.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: Kamalaruban15
month: 0
firstpage: 1035
lastpage: 1065
page: 1035-1065
sections: 
author:
- given: Parameswaran
  family: Kamalaruban
- given: Robert
  family: Williamson
- given: Xinhua
  family: Zhang
date: 2015-06-26
address: Paris, France
publisher: PMLR
container-title: Proceedings of The 28th Conference on Learning Theory
volume: '40'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 26
pdf: http://proceedings.mlr.press/v40/Kamalaruban15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
