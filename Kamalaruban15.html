<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Exp-Concavity of Proper Composite Losses | COLT 2015 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Exp-Concavity of Proper Composite Losses">

  <meta name="citation_author" content="Kamalaruban, Parameswaran">

  <meta name="citation_author" content="Williamson, Robert">

  <meta name="citation_author" content="Zhang, Xinhua">

<meta name="citation_publication_date" content="2015">
<meta name="citation_conference_title" content="Proceedings of The 28th Conference on Learning Theory">
<meta name="citation_firstpage" content="1035">
<meta name="citation_lastpage" content="1065">
<meta name="citation_pdf_url" content="Kamalaruban15.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Exp-Concavity of Proper Composite Losses</h1>

	<div id="authors">
	
		Parameswaran Kamalaruban,
	
		Robert Williamson,
	
		Xinhua Zhang
	<br />
	</div>
	<div id="info">
		Proceedings of The 28th Conference on Learning Theory,
		pp. 1035â€“1065, 2015
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		The goal of online prediction with expert advice is to find a decision strategy which will perform almost as well as the best expert in a given pool of experts, on any sequence of outcomes. This problem has been widely studied and <span class="math">\(O(\sqrt{T})\)</span> and <span class="math">\(O(\log{T})\)</span> regret bounds can be achieved for convex losses and strictly convex losses with bounded first and second derivatives respectively. In special cases like the Aggregating Algorithm with mixable losses and the Weighted Average Algorithm with exp-concave losses, it is possible to achieve <span class="math">\(O(1)\)</span> regret bounds. But mixability and exp-concavity are roughly equivalent under certain conditions. Thus by understanding the underlying relationship between these two notions we can gain the best of both algorithms (strong theoretical performance guarantees of the Aggregating Algorithm and the computational efficiency of the Weighted Average Algorithm). In this paper we provide a complete characterization of the exp-concavity of any proper composite loss. Using this characterization and the mixability condition of proper losses, we show that it is possible to transform (re-parameterize) any <span class="math">\(\beta\)</span>-mixable binary proper loss into a <span class="math">\(\beta\)</span>-exp-concave composite loss with the same <span class="math">\(\beta\)</span>. In the multi-class case, we propose an approximation approach for this transformation.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="Kamalaruban15.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
