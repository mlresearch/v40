---
title: Lower and Upper Bounds on the Generalization of  Stochastic   Exponentially
  Concave Optimization
abstract: In this paper we derive \textithigh probability lower and upper  bounds
  on the excess risk of stochastic optimization of exponentially concave loss functions.
  Exponentially  concave loss functions encompass several fundamental problems in
  machine learning such as squared loss in linear regression, logistic loss in classification,
  and negative logarithm loss in portfolio management.  We demonstrate an O(d \log
  T/T) upper bound on the excess risk of stochastic online Newton step algorithm,
  and an O(d/T) lower bound on the excess risk of any stochastic  optimization method
  for \textitsquared loss, indicating that the obtained upper bound  is  optimal up
  to a logarithmic factor. The analysis of upper bound is based on recent advances
  in  concentration inequalities for bounding self-normalized martingales, which is
  interesting by its own right, and the proof technique used to achieve the lower
  bound is a probabilistic method and relies on an information-theoretic minimax analysis.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: Mahdavi15
month: 0
firstpage: 1305
lastpage: 1320
page: 1305-1320
sections: 
author:
- given: Mehrdad
  family: Mahdavi
- given: Lijun
  family: Zhang
- given: Rong
  family: Jin
date: 2015-06-26
address: Paris, France
publisher: PMLR
container-title: Proceedings of The 28th Conference on Learning Theory
volume: '40'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 26
pdf: http://proceedings.mlr.press/v40/Mahdavi15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
