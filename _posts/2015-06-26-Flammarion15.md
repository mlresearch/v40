---
title: From Averaging to Acceleration, There is Only a Step-size
abstract: We show that accelerated gradient descent, averaged gradient descent and
  the heavy-ball method for quadratic non-strongly-convex problems may be reformulated
  as   constant parameter second-order difference equation algorithms, where stability
  of the system is equivalent to convergence at rate O(1/n^2), where n is the number
  of iterations. We provide a detailed analysis of the eigenvalues of the corresponding
  linear dynamical system, showing various oscillatory and non-oscillatory behaviors,
  together with a sharp stability result with explicit constants. We also consider
  the situation where noisy gradients are available, where we extend our general convergence
  result, which suggests an alternative algorithm (i.e., with different step sizes)
  that exhibits the good aspects of both averaging and acceleration.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: Flammarion15
month: 0
firstpage: 658
lastpage: 695
page: 658-695
sections: 
author:
- given: Nicolas
  family: Flammarion
- given: Francis
  family: Bach
date: 2015-06-26
address: Paris, France
publisher: PMLR
container-title: Proceedings of The 28th Conference on Learning Theory
volume: '40'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 26
pdf: http://proceedings.mlr.press/v40/Flammarion15/Flammarion15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
