---
title: Generalized Mixability via Entropic Duality
abstract: Mixability is a property of a loss which characterizes when constant regret
  is possible in the game of prediction with expert advice. We show that a key property
  of mixability generalizes, and the \exp and \log operations present in the usual
  theory are not as special as one might have thought.  In doing so we introduce a
  more general notion of Φ-mixability where Φis a general entropy (\emphi.e., any
  convex function on probabilities). We show how a property shared by the convex dual
  of any such entropy yields a natural algorithm (the minimizer of a regret bound)
  which, analogous to the classical Aggregating Algorithm, is guaranteed a constant
  regret when used with Φ-mixable losses.  We characterize which Φhave non-trivial
  Φ-mixable losses and relate Φ-mixability and its associated Aggregating Algorithm
  to potential-based methods, a Blackwell-like condition, mirror descent, and risk
  measures from finance.  We also define a notion of “dominance” between different
  entropies in terms of bounds they guarantee and conjecture that classical mixability
  gives optimal bounds, for which we provide some supporting empirical evidence.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: Reid15
month: 0
tex_title: Generalized Mixability via Entropic Duality
firstpage: 1501
lastpage: 1522
page: 1501-1522
sections: 
author:
- given: Mark D.
  family: Reid
- given: Rafael M.
  family: Frongillo
- given: Robert C.
  family: Williamson
- given: Nishant
  family: Mehta
date: 2015-06-26
address: Paris, France
publisher: PMLR
container-title: Proceedings of The 28th Conference on Learning Theory
volume: '40'
genre: inproceedings
issued:
  date-parts:
  - 2015
  - 6
  - 26
pdf: http://proceedings.mlr.press/v40/Reid15.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
