<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>On Consistent Surrogate Risk Minimization and Property Elicitation | COLT 2015 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="On Consistent Surrogate Risk Minimization and Property Elicitation">

  <meta name="citation_author" content="Agarwal, Arpit">

  <meta name="citation_author" content="Agarwal, Shivani">

<meta name="citation_publication_date" content="2015">
<meta name="citation_conference_title" content="Proceedings of The 28th Conference on Learning Theory">
<meta name="citation_firstpage" content="4">
<meta name="citation_lastpage" content="22">
<meta name="citation_pdf_url" content="Agarwal15.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>On Consistent Surrogate Risk Minimization and Property Elicitation</h1>

	<div id="authors">
	
		Arpit Agarwal,
	
		Shivani Agarwal
	<br />
	</div>
	<div id="info">
		Proceedings of The 28th Conference on Learning Theory,
		pp. 4–22, 2015
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		Surrogate risk minimization is a popular framework for supervised learning; property elicitation is a widely studied area in probability forecasting, machine learning, statistics and economics. In this paper, we connect these two themes by showing that calibrated surrogate losses in supervised learning can essentially be viewed as eliciting or estimating certain properties of the underlying conditional label distribution that are sufficient to construct an optimal classifier under the target loss of interest. Our study helps to shed light on the design of convex calibrated surrogates. We also give a new framework for designing convex calibrated surrogates under low-noise conditions by eliciting properties that allow one to construct ‘coarse’ estimates of the underlying distribution.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="Agarwal15.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
