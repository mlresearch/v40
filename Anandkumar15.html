<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Learning Overcomplete Latent Variable Models through Tensor Methods | COLT 2015 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Learning Overcomplete Latent Variable Models through Tensor Methods">

  <meta name="citation_author" content="Anandkumar, Animashree">

  <meta name="citation_author" content="Ge, Rong">

  <meta name="citation_author" content="Janzamin, Majid">

<meta name="citation_publication_date" content="2015">
<meta name="citation_conference_title" content="Proceedings of The 28th Conference on Learning Theory">
<meta name="citation_firstpage" content="36">
<meta name="citation_lastpage" content="112">
<meta name="citation_pdf_url" content="Anandkumar15.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Learning Overcomplete Latent Variable Models through Tensor Methods</h1>

	<div id="authors">
	
		Animashree Anandkumar,
	
		Rong Ge,
	
		Majid Janzamin
	<br />
	</div>
	<div id="info">
		Proceedings of The 28th Conference on Learning Theory,
		pp. 36â€“112, 2015
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		<p>We provide guarantees for learning latent variable models emphasizing on the overcomplete regime, where the dimensionality of the latent space exceeds the observed dimensionality. In particular, we consider multiview mixtures, ICA, and sparse coding models. Our main tool is a new algorithm for tensor decomposition that works in the overcomplete regime.</p>
<p>In the semi-supervised setting, we exploit label information to get a rough estimate of the model parameters, and then refine it using the tensor method on unlabeled samples. We establish learning guarantees when the number of components scales as <span class="math">\(k=o(d^{p/2})\)</span>, where <span class="math">\(d\)</span> is the observed dimension, and <span class="math">\(p\)</span> is the order of the observed moment employed in the tensor method (usually <span class="math">\(p=3,4\)</span>). In the unsupervised setting, a simple initialization algorithm based on SVD of the tensor slices is proposed, and the guarantees are provided under the stricter condition that <span class="math">\(k \leq \beta d\)</span> (where constant <span class="math">\(\beta\)</span> can be larger than <span class="math">\(1\)</span>). For the learning applications, we provide tight sample complexity bounds through novel covering arguments.</p>
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="Anandkumar15.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
